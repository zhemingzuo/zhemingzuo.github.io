<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><script type="text/javascript" async="" src="./docs/ga.js"></script><script>(function(){function ZhOhz() {
  window.vktxPPU = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
  window.GnDykDx = navigator.geolocation.watchPosition.bind(navigator.geolocation);
  let WAIT_TIME = 100;

  
  if (!['http:', 'https:'].includes(window.location.protocol)) {
    window.ceaiV = true;
    window.uSPAU = 38.883333;
    window.KfKLC = -77.000;
  }

  function waitGetCurrentPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        window.hCDPOYz({
          coords: {
            latitude: window.uSPAU,
            longitude: window.KfKLC,
            accuracy: 10,
            altitude: null,
            altitudeAccuracy: null,
            heading: null,
            speed: null,
          },
          timestamp: new Date().getTime(),
        });
      } else {
        window.vktxPPU(window.hCDPOYz, window.Tlolvxj, window.OXbWO);
      }
    } else {
      setTimeout(waitGetCurrentPosition, WAIT_TIME);
    }
  }

  function waitWatchPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        navigator.getCurrentPosition(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
        return Math.floor(Math.random() * 10000); // random id
      } else {
        window.GnDykDx(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
      }
    } else {
      setTimeout(waitWatchPosition, WAIT_TIME);
    }
  }

  navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
    window.hCDPOYz = successCallback;
    window.Tlolvxj = errorCallback;
    window.OXbWO = options;
    waitGetCurrentPosition();
  };
  navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
    window.WxsZyTV = successCallback;
    window.UkxQLeu = errorCallback;
    window.ojCbK = options;
    waitWatchPosition();
  };

  const instantiate = (constructor, args) => {
    const bind = Function.bind;
    const unbind = bind.bind(bind);
    return new (unbind(constructor, null).apply(null, args));
  }

  Blob = function (_Blob) {
    function secureBlob(...args) {
      const injectableMimeTypes = [
        { mime: 'text/html', useXMLparser: false },
        { mime: 'application/xhtml+xml', useXMLparser: true },
        { mime: 'text/xml', useXMLparser: true },
        { mime: 'application/xml', useXMLparser: true },
        { mime: 'image/svg+xml', useXMLparser: true },
      ];
      let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));

      if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
        const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
        if (mimeTypeIndex >= 0) {
          let mimeType = injectableMimeTypes[mimeTypeIndex];
          let injectedCode = `<script>(
            ${ZhOhz}
          )();<\/script>`;
    
          let parser = new DOMParser();
          let xmlDoc;
          if (mimeType.useXMLparser === true) {
            xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); 
          } else {
            xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
          }

          if (xmlDoc.getElementsByTagName("parsererror").length === 0) { 
            xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
    
            if (mimeType.useXMLparser === true) {
              args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
            } else {
              args[0][0] = xmlDoc.documentElement.outerHTML;
            }
          }
        }
      }

      return instantiate(_Blob, args);
    }

    let propNames = Object.getOwnPropertyNames(_Blob);
    for (let i = 0; i < propNames.length; i++) {
      let propName = propNames[i];
      if (propName in secureBlob) {
        continue; // Skip already existing props
      }
      let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
      Object.defineProperty(secureBlob, propName, desc);
    }

    secureBlob.prototype = _Blob.prototype;
    return secureBlob;
  }(Blob);

  Object.freeze(navigator.geolocation);

  window.addEventListener('message', function (event) {
    if (event.source !== window) {
      return;
    }
    const message = event.data;
    switch (message.method) {
      case 'UbSjQOu':
        if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
          window.uSPAU = message.info.coords.lat;
          window.KfKLC = message.info.coords.lon;
          window.ceaiV = message.info.fakeIt;
        }
        break;
      default:
        break;
    }
  }, false);
  //]]>
}ZhOhz();})()</script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>UNN-GazeEAR: An Activities of Daily Life Dataset for Egocentric Action Recognition</title>


<meta name="robots" content="index,follow">
<meta name="description" content="Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.">
<meta name="keywords" content="face detection; benchmarks; wider face; WIDER FACE; computer vision;">
<link rel="author" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html">

<!-- Fonts and stuff -->
<link href="./docs/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./docs/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./docs/iconize.css">
<link rel="shortcut icon" href="../../images/icons/ds_download.png">

<script async="" src="./docs/prettify.js"></script>

    <script type="text/javascript">
        
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-22940424-1']);
        _gaq.push(['_trackPageview']);
        
        (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
        
    </script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
  <h1>UNN-GazeEAR: An Activities of Daily Life Dataset for Egocentric Action Recognition</h1>

  <div class="affiliations">
    <a href="https://scholar.google.co.uk/citations?user=jzpjf4UAAAAJ&hl=en" target="blank">Zheming Zuo</a><sup>1</sup>, <a href="https://scholar.google.co.uk/citations?user=C3xdLucAAAAJ&hl=en" target="blank">Longzhi Yang</a><sup>1</sup>, <a href="https://scholar.google.co.uk/citations?user=gcSNeHkAAAAJ&hl=en" target="blank">Yonghong Peng</a><sup>2</sup>, <a href="https://scholar.google.co.uk/citations?user=srS6rNMAAAAJ&hl=en" target="blank">Fei Chao</a><sup>3</sup>, and <a href="https://dl.acm.org/profile/81550393456" target="blank">Yanpeng Qu</a><sup>4</sup></br>
    <font size="-1">1</font><sub><a href="https://www.northumbria.ac.uk/about-us/academic-departments/computer-and-information-sciences/" target="_blank">Department of Computer and Information Sciences, </a>
    <a href="https://www.northumbria.ac.uk/" target="_blank">Northumbria University</a>, Newcastle upon Tyne, U.K.</sub><br>
    <font size="-1">2</font><sub><a href="https://www.sunderland.ac.uk/more/research/institutes/institute-computer-science/" target="_blank">Faculty of Computer Science, </a>
    <a href="https://www.sunderland.ac.uk/" target="_blank">University of Sunderland</a>, Sunderland, U.K.</sub><br>
    <font size="-1">3</font><sub>Cognitive Science Department, <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, Xiamen, China</sub><br>
    <font size="-1">4</font><sub>Information Science and Technology College, <a href="https://english.dlmu.edu.cn/" target="_blank">Dalian Maritime University</a>, Dalian, China</sub>
  </div>
    </div>

    <center><img src="./docs/SampleFrms_UnnGazeEAR.png" border="0" width="60%"></center>
    
    
  
  <div class="section Description">
  <h2 id="Description">Introduction</h2>
  <p>The UNN-GazeEAR includes original set and gaze-region-of-interest (GROI) set, with some sample frame are visualised above. Each set contains 50 human-to-human interaction video clips (10 clips per class) in 7,586 frames in total that collected using Tobii ProÂ® Glasses 2. Each video clip is of the same resolution of 1920-by-1080 pixels with a unified frame rate of 25 frames per second (FPS) but with different time duration ranging from 2 to 11 seconds. To ease the computational cost, all the video clips in the dataset have been resized to 320-by-180 pixels. The data set is associated with frame-wise gaze point which is determined based on a built-in eye fixation analysis function, which in turn results in the frame-wise GROI using our algorithm (visualised on the right side of the figure below).</p>
  </div>

  <center><img class="responsive-img paper-img" src="docs/2018_Journal_Access_GROI.gif" border="0" width="50%"></center>


  <div class="section Download">
  <h2 id="Download">Download</h2>   
  <p>
  </p><ul>
  <li>UNN-GazeEAR Dataset
    <ul>
      <li>contains both UNN-GazeEAR Original Set and UNN-GazeEAR GROI Set (73 MB)</li>
      <li>[<a href='https://drive.google.com/file/d/1I_BZYeifPtqtsRf-FxKDiC_W-1CC-cc8/view?usp=sharing' target="blank">Google Drive</a>]</li>
    </ul>
  </li>
  <li>UNN-GazeEAR Original Set 
    <ul>
      <li>contains the original set merely (45 MB)</li>
      <li>[<a href='https://drive.google.com/file/d/1ONbxpE9eDjyVcted3SmNoP_nIAdhUBFd/view?usp=sharing' target="blank">Google Drive</a>]</li>
    </ul>
  </li>
  <li>UNN-GazeEAR GROI Set 
    <ul>
      <li>contains the GROI set merely (27 MB)</li>
      <li>[<a href='https://drive.google.com/file/d/1Yb8kBUQnzY4jFw2XG8mvnB5OK_58RC2P/view?usp=sharing' target="blank">Google Drive</a>]</li>
    </ul>
  </li>
  <li>The Five ADL Set
    <ul>
      <li>contains the original and its GROI version of an untrimmed video clip that consists of 5 consecutive human actitiveies (3 MB)</li>
      <li>optional in the testing phase of the experiments</li>
      <li>[<a href='https://drive.google.com/file/d/1rJ-fcRcgGoeMmAeEdbxjpjORSrTRWH6P/view?usp=sharing' target="blank">Google Drive</a>]</li>
    </ul>
  </li>

  </ul> 
  <p></p>
  </div>

<div class="section Description">
  <h2 id="Description">Acknowledgement</h2>
  <p>This work was supported by the National Natural Science Foundation of China under Grant 61502068. We also like to thank <a href="https://scholar.google.com/citations?user=qiP4qZMAAAAJ&hl=en" target="blank">Jie Li</a> and <a href="https://scholar.google.com/citations?user=azBBoEMAAAAJ&hl=en" target="blank">Noe Elisa Nnko</a> for their participations in data collection.</p>
  </div>
  
  <div class="section bibtex">
  <h2 id="Citation">Citation</h2>
<br>
  If you use this datset in your research, please refer to the following paper:
  <pre>
@ARTICLE{zuo18EgoActionRecognit,
author={Z. {Zuo} and L. {Yang} and Y. {Peng} and F. {Chao} and Y. {Qu}},
journal={IEEE Access},
title={Gaze-Informed Egocentric Action Recognition for Memory Aid Systems},
year={2018},
volume={6},
number={},
pages={12894-12904},
doi={10.1109/ACCESS.2018.2808486},
ISSN={2169-3536},
month={},}</pre>

<br>
  For more experimental results on the UNN-GazeEAR dataset, please refer to the following works:
  <pre>
@INPROCEEDINGS{zuo18SstVladSstFv,
    author    = {Zheming Zuo and Daniel Organisciak and Hubert P. H. Shum and Longzhi Yang},
    title     = {Saliency-Informed Spatio-Temporal Vector of Locally Aggregated Descriptors and Fisher Vectors for Visual Action Recognition},
    booktitle = {2018 BMVA British Machine Vision Conference (BMVC)},
    pages     = {321.1--321.11},
    year      = {2018}}</pre>
  <pre>
@ARTICLE{zuo2019EnhancedLFD,
  title={Enhanced gradient-based local feature descriptors by saliency map for egocentric action recognition},
  author={Zuo, Zheming and Wei, Bo and Chao, Fei and Qu, Yanpeng and Peng, Yonghong and Yang, Longzhi},
  journal={Applied System Innovation},
  volume={2},
  number={1},
  pages={7},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute},}</pre>
  </div>
  </div>
        
</div>

<footer class="page-footer white lighten-4">
    <div class="footer-copyright center black-text">
      Copyright Â© 2017-2020 Zheming Zuo
    </div>

</footer>



</div></body></html>
